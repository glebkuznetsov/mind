{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"Agg\")\n",
    "import matplotlib.patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import transforms\n",
    "from multiprocessing import Pool\n",
    "from statsmodels.sandbox.stats import multicomp\n",
    "\n",
    "sys.path.append(\"utils\")\n",
    "import pyximport; pyximport.install(setup_args={\"include_dirs\":np.get_include()},\n",
    "                                    reload_support=True)\n",
    "import cython_fnx\n",
    "from utils import *\n",
    "from plotting_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. define & collect arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-s\", \"--seed\", type=int, required=False, default=12345, \n",
    "                    help=\"numpy.random seed to use (for reproducibility)\")\n",
    "parser.add_argument(\"-p\", \"--pwm_file\", type=str, required=True, \n",
    "                    help=\"path to pwm file in MEME format\")\n",
    "parser.add_argument(\"-d\", \"--deletion_info_file\", type=str, required=True, \n",
    "                    help=\"path to file containing list of deletion files to analyze (full paths)\")\n",
    "parser.add_argument(\"-l\", \"--seq_len\", type=int, required=True, \n",
    "                    help=\"length of sequences used in deletion MPRA\")\n",
    "parser.add_argument(\"-f\", \"--offset\", type=int, required=False, default=1,\n",
    "                    help=\"# bp that the start number in deletion files is offset from 0\")\n",
    "parser.add_argument(\"-w\", \"--bandwidth\", type=int, required=False, default=5,\n",
    "                    help=\"bandwidth to use in moving average smoother (NOTE: should be odd)\")\n",
    "parser.add_argument(\"-p\", \"--peak_cutoff\", type=float, required=False, default=0.5,\n",
    "                    help=\"cutoff to use when calling peaks\")\n",
    "parser.add_argument(\"-t\", \"--score_type\", type=str, required=True, \n",
    "                    help=\"either 'loss' or 'gain'\")\n",
    "parser.add_argument(\"-n\", \"--n_shuffles\", type=int, required=False, default=1000,\n",
    "                    help=\"# times to shuffle peak data to get null distribution\")\n",
    "parser.add_argument(\"-e\", \"--tfs_expressed_file\", type=str, required=False, default=None, \n",
    "                    help=\"path to file containing list of TFs expressed in cell line of interest\")\n",
    "parser.add_argument(\"-c\", \"--cores\", type=int, required=True,\n",
    "                    help=\"# cores to use when computing\")\n",
    "parser.add_argument(\"-o\", \"--out_dir\", type=str, required=True, \n",
    "                    help=\"directory where results will be stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args()\n",
    "seed = args.seed\n",
    "pwm_file = args.pwm_file\n",
    "deletion_info_file = args.deletion_info_file\n",
    "seq_len = args.seq_len\n",
    "offset = args.offset\n",
    "bandwidth = args.bandwidth\n",
    "peak_cutoff = args.peak_cutoff\n",
    "score_type = args.score_type\n",
    "n_shuffles = args.n_shuffles\n",
    "tfs_expressed_file = args.tfs_expressed_file\n",
    "cores = args.cores\n",
    "out_dir = args.out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defaults for debugging\n",
    "# seed = 12345\n",
    "# pwm_file = \"inputs/0__pwm/pfm_vertebrates_meme_motifNameChanged.txt\"\n",
    "# deletion_info_file = \"inputs/1__dels/deletion_files.txt\"\n",
    "# seq_len = 94\n",
    "# offset = 1\n",
    "# bandwidth = 5\n",
    "# peak_cutoff = 0.5\n",
    "# score_type = \"loss\"\n",
    "# n_shuffles = 1000\n",
    "# tfs_expressed_file = None\n",
    "# cores = 4\n",
    "# out_dir = \"results/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### argument assertions ###\n",
    "\n",
    "# pwm file exists\n",
    "assert os.path.exists(pwm_file), \"--pwm_file path does not exist\"\n",
    "\n",
    "# deletion file exists\n",
    "assert os.path.exists(deletion_info_file), \"--deletion_info_file path does not exist\"\n",
    "\n",
    "# bandwidth is odd\n",
    "assert bandwidth % 2 == 1, \"--bandwidth should be an odd number\"\n",
    "\n",
    "# score is either loss or gain\n",
    "assert score_type in [\"loss\", \"gain\"], \"--score_type should be either 'loss' or 'gain'\"\n",
    "\n",
    "# tf file exists if given\n",
    "if tfs_expressed_file != None:\n",
    "    assert os.path.exists(tfs_expressed_file), \"--tfs_expressed_file path does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### set plotting defaults ###\n",
    "sns.set(**PRESET)\n",
    "fontsize = FONTSIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. import data & make out dir if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibility!\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in pwm file\n",
    "motifs, motif_lens, motif_len_map = parse_pfm(pwm_file, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find max motif length\n",
    "max_motif_len = np.max(list(motif_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in file with paths to all of the deletion data\n",
    "deletion_info = pd.read_table(deletion_info_file, sep=\"\\t\", header=None)\n",
    "deletion_info.columns = [\"path\", \"name\"]\n",
    "deletion_info[\"path\"] = deletion_info[\"path\"].map(str.strip)\n",
    "deletion_info[\"name\"] = deletion_info[\"name\"].map(str.strip)\n",
    "deletion_info = zip(list(deletion_info[\"path\"]), list(deletion_info[\"name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in all of the deletion data and make sure it has the columns we need\n",
    "data = {}\n",
    "for path, name in deletion_info:\n",
    "    \n",
    "    assert os.path.exists(path), \"path to deletion data %s does not exist\" % path\n",
    "    df = pd.read_table(path, sep=\"\\t\")\n",
    "    \n",
    "    assert \"delpos\" in df.columns, \"deletion file %s does not have 'delpos' as a column name\" % path\n",
    "    assert \"seq\" in df.columns, \"deletion file %s does not have 'seq' as a column name\" % path\n",
    "    assert \"mean.log2FC\" in df.columns, \"deletion file %s does not have 'mean.log2FC' as a column name\" % path\n",
    "    assert \"sd\" in df.columns, \"deletion file %s does not have 'sd' as a column name\" % path\n",
    "    assert \"se\" in df.columns, \"deletion file %s does not have 'se' as a column name\" % path\n",
    "    \n",
    "    data[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the tfs expressed file, if it exists\n",
    "if tfs_expressed_file != None:\n",
    "    tfs_expressed = pd.read_table(tfs_expressed_file, sep=\"\\t\", header=None)\n",
    "    tfs_expressed.columns = [\"tf\"]\n",
    "    tfs_expressed = list(tfs_expressed[\"tf\"])\n",
    "else:\n",
    "    tfs_expressed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make subdir for results files\n",
    "res_dir = \"%s/files\" % out_dir\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "    \n",
    "# make subdir for figures\n",
    "figs_dir = \"%s/figs\" % out_dir\n",
    "if not os.path.exists(figs_dir):\n",
    "    os.makedirs(figs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. create loss or gain score\n",
    "loss = looking for transcriptional activators; gain = looking for transcriptional repressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for seq in data.keys():\n",
    "    df = data[seq]\n",
    "\n",
    "    if score_type == \"loss\":\n",
    "        df[\"loss_score_raw\"] = df.apply(loss_score, col=\"mean.log2FC\", axis=1)\n",
    "        \n",
    "        # scale scores (only if there are scores > 2, otherwise don't)\n",
    "        scores = list(df[\"loss_score_raw\"])\n",
    "        scores.extend([0, 2])\n",
    "        scaled_scores = scale_range(scores, 0, 2)\n",
    "        del scaled_scores[-2:]\n",
    "        df[\"loss_score_raw_scaled\"] = scaled_scores\n",
    "        \n",
    "    elif score_type == \"gain\":\n",
    "        df[\"gain_score_raw\"] = df.apply(gain_score, col=\"mean.log2FC\", axis=1)\n",
    "        \n",
    "        # scale scores (only if there are scores > 2, otherwise don't)\n",
    "        scores = list(df[\"gain_score_raw\"])\n",
    "        scores.extend([0, 2])\n",
    "        scaled_scores = scale_range(scores, 0, 2)\n",
    "        del scaled_scores[-2:]\n",
    "        df[\"gain_score_raw_scaled\"] = scaled_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. find peaks in the sequences and write files w/ peak info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make subdir for peak figures\n",
    "peak_figs_dir = \"%s/0__peaks\" % figs_dir\n",
    "if not os.path.exists(peak_figs_dir):\n",
    "    os.makedirs(peak_figs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make subdir for peak results\n",
    "peak_res_dir = \"%s/0__ntd_scores\" % res_dir\n",
    "if not os.path.exists(peak_res_dir):\n",
    "    os.makedirs(peak_res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting: ZBTB37__p1__tile1__plus\n",
      "plotting: ZFAS1__p1__tile1__plus\n"
     ]
    }
   ],
   "source": [
    "score_col = \"%s_score_raw_scaled\" % (score_type)\n",
    "data_peaks = {}\n",
    "peak_dfs = {}\n",
    "\n",
    "for seq in data.keys():\n",
    "    print(\"plotting: %s\" % seq)\n",
    "    seq_name = \"%s__%s\" % (seq, score_type)\n",
    "\n",
    "    # extract bases & scores from df\n",
    "    df = data[seq]\n",
    "    bases = list(df[\"seq\"])\n",
    "    scaled_scores = list(df[score_col])\n",
    "    yerrs = list(df[\"se\"])\n",
    "    raw_scores = list(df[\"mean.log2FC\"])\n",
    "    \n",
    "    # apply moving average to scores\n",
    "    scores_filt = moving_average(bandwidth, scaled_scores)\n",
    "    df[\"filtered_score\"] = scores_filt\n",
    "    \n",
    "    # find peaks\n",
    "    widths, peak_info, df = find_peaks(peak_cutoff, seq_len, df, \n",
    "                                       scores_filt, scaled_scores, bases, offset, \n",
    "                                       max_motif_len)\n",
    "    data_peaks[seq] = peak_info\n",
    "    peak_dfs[seq] = df\n",
    "    \n",
    "    # write file\n",
    "    df.to_csv(\"%s/%s.%s_score.txt\" % (peak_res_dir, seq, score_type), sep=\"\\t\", index=False)\n",
    "    \n",
    "    # plot peaks\n",
    "    plot_peaks((4.9, 1.4), 6, score_type, seq_len, seq_name, bandwidth, widths, raw_scores, yerrs, scores_filt, \n",
    "               scaled_scores, bases, peak_figs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. find motifs in the peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mapping motifs...\n",
      "using multiprocessing\n",
      "seq name: ZBTB37__p1__tile1__plus | n peaks: 0\n",
      "seq name: ZFAS1__p1__tile1__plus | n peaks: 1\n",
      "\n",
      "getting results...\n",
      "ZBTB37__p1__tile1__plus (Thu May 31 17:45:06 2018)\n",
      "ZFAS1__p1__tile1__plus (Thu May 31 17:45:06 2018)\n",
      "=================\n",
      "time elapsed: 67.46668601036072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"mapping motifs...\")\n",
    "results_dict = get_all_results(cores, data_peaks, motifs, n_shuffles, seed, parallel=True)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. correct results for mult. hyp. & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make subdir for results figures\n",
    "res_figs_dir = \"%s/1__results\" % figs_dir\n",
    "if not os.path.exists(res_figs_dir):\n",
    "    os.makedirs(res_figs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make subdir for results figures\n",
    "motif_res_dir = \"%s/1__motif_scores\" % res_dir\n",
    "if not os.path.exists(motif_res_dir):\n",
    "    os.makedirs(motif_res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZFAS1__p1__tile1__plus__peak1\n",
      "ALL MOTIFS: found 8 motifs at 0.15 FDR\n"
     ]
    }
   ],
   "source": [
    "# check motifs at 3 FDRs (since every peak is different)\n",
    "alphas = [0.05, 0.1, 0.15]\n",
    "\n",
    "for seq in results_dict:\n",
    "    seq_results = results_dict[seq]\n",
    "    n_peaks = len(seq_results)\n",
    "    for p in range(n_peaks):\n",
    "        name = \"%s__peak%s\" % (seq, p+1)\n",
    "        print(name)\n",
    "        peak_results = seq_results[p]\n",
    "        deduped_dict = {}\n",
    "\n",
    "        # for every motif, choose the max of either sense or antisense pwm\n",
    "        for motif in peak_results:\n",
    "            sense_score = peak_results[motif][0][2]\n",
    "            antisense_score = peak_results[motif][1][2]\n",
    "            if sense_score >= antisense_score:\n",
    "                max_row = peak_results[motif][0]\n",
    "                max_row.extend([\"sense\"])\n",
    "            else:\n",
    "                max_row = peak_results[motif][1]\n",
    "                max_row.extend([\"antisense\"])\n",
    "            max_row_fixed = list(max_row[0:8])\n",
    "            deduped_dict[motif] = max_row_fixed\n",
    "            \n",
    "        df = pd.DataFrame.from_dict(deduped_dict, orient=\"index\").reset_index()\n",
    "        df.columns = [\"motif\", \"start\", \"end\", \"score\", \"pval\", \"tile_chr\", \"tile_start\", \n",
    "                      \"tile_end\", \"strand\"]\n",
    "        \n",
    "        # correct p values for multiple testing\n",
    "        padj = multicomp.multipletests(list(df[\"pval\"]), method=\"fdr_bh\")[1]\n",
    "        df[\"padj\"] = padj\n",
    "        df[\"neg_log_pval\"] = -np.log10(df[\"padj\"])\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            sig_df = df[df[\"padj\"] < alpha]\n",
    "            if len(sig_df) > 0:\n",
    "                break\n",
    "        print(\"ALL MOTIFS: found %s motifs at %s FDR\" % (len(sig_df), alpha))\n",
    "        df[\"fdr_cutoff\"] = alpha\n",
    "        df = df.sort_values(by=\"score\", ascending=False)\n",
    "        df.to_csv(\"%s/%s.%s__results.txt\" % (res_dir, name, score_type), sep=\"\\t\", index=False)\n",
    "\n",
    "        \n",
    "        # if a list of TFs expressed was provided, filter to those only & adjust those only\n",
    "        if tfs_expressed != None:\n",
    "            df_sub = df[df[\"motif\"].isin(tfs_on)]\n",
    "        \n",
    "            padj = multicomp.multipletests(list(df_sub[\"pval\"]), method=\"fdr_bh\")[1]\n",
    "            df_sub[\"padj\"] = padj\n",
    "            df_sub[\"neg_log_pval\"] = -np.log10(df_sub[\"padj\"])\n",
    "        \n",
    "        \n",
    "            for alpha in alphas:\n",
    "                sig_df_sub = df_sub[df_sub[\"padj\"] < alpha]\n",
    "                if len(sig_df_sub) > 0:\n",
    "                    break\n",
    "            print(\"ONLY TFS EXPRESSED: found %s motifs at %s FDR\" % (len(sig_df), alpha))\n",
    "            df_sub[\"fdr_cutoff\"] = alpha\n",
    "            df_sub = df_sub.sort_values(by=\"score\", ascending=False)\n",
    "            df_sub.to_csv(\"%s/%s.%s__results.expr_filt.txt\" % (res_dir, name, score_type), sep=\"\\t\", \n",
    "                          index=False)\n",
    "        \n",
    "        # plot the all results only\n",
    "        plot_motif_results(df, 2.5, name, alpha, res_figs_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
